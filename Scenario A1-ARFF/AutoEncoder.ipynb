{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c679d77f-bf43-49d9-b7ed-368d7f3eec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4b498d-9a95-4ec9-aa92-a73002cf8b1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScenario A1-ARFF\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTimeBasedFeatures-Dataset-15s-VPN.arff\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n\u001b[0;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _loadarff(ofile)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "filepath = r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff'\n",
    "\n",
    "# Daten laden\n",
    "raw_data, meta = arff.loadarff(filepath)\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "\n",
    "df['class1'] = df['class1'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "\n",
    "selected_features = [\n",
    "    'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "    'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "    'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "    'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "]\n",
    "df_selected = df[selected_features]\n",
    "labels = df['class1']\n",
    "\n",
    "\n",
    "df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_selected)\n",
    "\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Trainingsdaten Shape:\", X_train.shape)\n",
    "print(\"Testdaten Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdce71f8-c479-481f-91f2-d17b741524f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Skalierung der Features mit StandardScaler\u001b[39;00m\n\u001b[0;32m     10\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 11\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     12\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Falls X_* noch DataFrames sind, in NumPy-Arrays umwandeln\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Skalierung der Features mit StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Falls X_* noch DataFrames sind, in NumPy-Arrays umwandeln\n",
    "if isinstance(X_train_scaled, pd.DataFrame):\n",
    "    X_train_np = X_train_scaled.values\n",
    "    X_test_np = X_test_scaled.values\n",
    "else:\n",
    "    X_train_np = X_train_scaled\n",
    "    X_test_np = X_test_scaled\n",
    "\n",
    "# Labels als NumPy-Array speichern\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Nur Non-VPN-Daten für das Training verwenden (unüberwachtes Lernen)\n",
    "mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "# Definition der Autoencoder-Architektur\n",
    "input_dim = X_train_norm.shape[1]\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "x = layers.Dense(128, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(96, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(48, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(48, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(96, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Kompilieren des Autoencoders mit Adamax-Optimizer und Huber-Loss\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=0.0007), loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "# Callbacks für Early Stopping und Learning Rate Anpassung\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "clr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Training des Autoencoders\n",
    "history = autoencoder.fit(\n",
    "    X_train_norm, X_train_norm,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    callbacks=[clr, early_stopping]\n",
    ")\n",
    "\n",
    "# Vorhersage des Autoencoders auf Testdaten\n",
    "X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "\n",
    "# Berechnung des Rekonstruktionsfehlers auf Trainingsdaten\n",
    "X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "\n",
    "# Festlegen des Schwellenwerts für Anomalie-Erkennung\n",
    "threshold = np.percentile(mse_train_norm, 65)\n",
    "\n",
    "y_pred = np.where(mse > threshold, 1, 0)  # 1 = Anomalie (VPN), 0 = Normal (Non-VPN)\n",
    "y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "# Berechnung der Confusion Matrix und der Klassifikationsmetriken\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nCONFUSION MATRIX:\\n\", cm)\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\",\"VPN\"])\n",
    "print(\"\\nCLASSIFICATION REPORT:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c714f15-4d62-4ec0-a2db-80b971d0117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starte Training für Datensatz: VPN\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.arff\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Starte Training für Datensatz: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n\u001b[0;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _loadarff(ofile)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'"
     ]
    }
   ],
   "source": [
    "# TEST ALL MODELLS (81 VARIANTEN)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filepaths = [\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff',\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-30s-VPN.arff',\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-60s-VPN.arff'\n",
    "]\n",
    "\n",
    "bottleneck_sizes = [16, 32, 64]\n",
    "learning_rates = [0.0005, 0.0007, 0.001]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "thresholds = [50, 65, 75]\n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for filepath in filepaths:\n",
    "    dataset_name = filepath.split('-')[-1].replace('.arff', '')\n",
    "    print(f\" Starte Training für Datensatz: {dataset_name}\")\n",
    "\n",
    "    raw_data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    selected_features = [\n",
    "        'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "        'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "        'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "        'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "    ]\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    X_train_np = X_train.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "\n",
    "    mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "    X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    for bottleneck in bottleneck_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                # Architektur definieren\n",
    "                input_dim = X_train_norm.shape[1]\n",
    "                input_layer = keras.Input(shape=(input_dim,))\n",
    "                x = layers.Dense(128, activation='elu')(input_layer)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(96, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(48, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(bottleneck, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(48, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(96, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(128, activation='elu')(x)\n",
    "                output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "                autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "                autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=lr),\n",
    "                                   loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "                autoencoder.fit(\n",
    "                    X_train_norm, X_train_norm,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "                mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "\n",
    "                X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "                mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "\n",
    "                for threshold_p in thresholds:\n",
    "                    threshold = np.percentile(mse_train_norm, threshold_p)\n",
    "                    print(f\" Testing: {dataset_name} | Bottleneck={bottleneck}, LR={lr}, Dropout={dropout_rate}, Threshold={threshold_p}%\")\n",
    "\n",
    "                    y_pred = np.where(mse > threshold, 1, 0)\n",
    "                    y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"], output_dict=True)\n",
    "                    f1_score = report[\"VPN\"][\"f1-score\"]\n",
    "\n",
    "                    if f1_score > best_f1:\n",
    "                        best_f1 = f1_score\n",
    "                        best_model = autoencoder\n",
    "                        best_params = (bottleneck, lr, dropout_rate, threshold_p)\n",
    "\n",
    "    best_results[dataset_name] = {\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_params\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Best Model for {dataset_name}:\")\n",
    "    print(f\"  - Bottleneck: {best_params[0]}\")\n",
    "    print(f\"  - Learning Rate: {best_params[1]}\")\n",
    "    print(f\"  - Dropout Rate: {best_params[2]}\")\n",
    "    print(f\"  - Threshold: {best_params[3]}%\")\n",
    "    print(f\"  - Best F1-Score: {best_f1:.4f}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74221ff5-8dde-48f7-a86a-9b4a71401ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starte Training & Test für Datensatz: 15s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m threshold_p \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreshold_p\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n\u001b[0;32m     45\u001b[0m df\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _loadarff(ofile)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "best_params = {\n",
    "    \"15s\": {\"bottleneck\": 16, \"lr\": 0.001, \"dropout\": 0.1, \"threshold_p\": 50},\n",
    "    \"30s\": {\"bottleneck\": 32, \"lr\": 0.0007, \"dropout\": 0.2, \"threshold_p\": 50},\n",
    "    \"60s\": {\"bottleneck\": 16, \"lr\": 0.0005, \"dropout\": 0.1, \"threshold_p\": 50}\n",
    "}\n",
    "\n",
    "\n",
    "filepaths = {\n",
    "    \"15s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff',\n",
    "    \"30s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-30s-VPN.arff',\n",
    "    \"60s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-60s-VPN.arff'\n",
    "}\n",
    "\n",
    "# Speicherort für Modelle\n",
    "model_dir = \"saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Trainiere & teste je Datensatz mit besten Hyperparametern\n",
    "for dataset_name, filepath in filepaths.items():\n",
    "    print(f\"\\n Starte Training & Test für Datensatz: {dataset_name}\")\n",
    "\n",
    "    # Beste Parameter für diesen Datensatz\n",
    "    params = best_params[dataset_name]\n",
    "    bottleneck = params[\"bottleneck\"]\n",
    "    learning_rate = params[\"lr\"]\n",
    "    dropout_rate = params[\"dropout\"]\n",
    "    threshold_p = params[\"threshold_p\"]\n",
    "\n",
    "    # Daten laden\n",
    "    raw_data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    # **Feature-Engineering**\n",
    "    selected_features = [\n",
    "        'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "        'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "        'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "        'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "    ]\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    # **Skalierung**\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    # **Train-Test-Split**\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    X_train_np = X_train.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "\n",
    "    mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "    X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "    # **Autoencoder Architektur**\n",
    "    input_dim = X_train_norm.shape[1]\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='elu')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(48, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(bottleneck, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(48, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='elu')(x)\n",
    "    output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=learning_rate),\n",
    "                        loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "    # Training des Autoencoders\n",
    "    autoencoder.fit(\n",
    "        X_train_norm, X_train_norm,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Threshold berechnen\n",
    "    X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "    mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "    threshold = np.percentile(mse_train_norm, threshold_p)\n",
    "\n",
    "    print(f\"\\n Training für {dataset_name} fertig! Schwellenwert = {threshold:.6f}\\n\")\n",
    "\n",
    "    # Modell testen\n",
    "    X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "    mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "    \n",
    "    y_pred = np.where(mse > threshold, 1, 0)\n",
    "    y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"])\n",
    "\n",
    "    print(f\"\\n Ergebnisse für {dataset_name}:\")\n",
    "    print(\"CONFUSION MATRIX:\\n\", cm)\n",
    "    print(\"CLASSIFICATION REPORT:\\n\", report)\n",
    "\n",
    "    # Modell speichern\n",
    "    model_path = os.path.join(model_dir, f\"autoencoder_{dataset_name}.h5\")\n",
    "    autoencoder.save(model_path)\n",
    "    print(f\" Modell gespeichert: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3647345a-7552-4817-92fd-f2192bca543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alle Modelle erfolgreich geladen!\n",
      "\n",
      " Teste Modell 15s auf den passenden Datensatz...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Teste Modell \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m auf den passenden Datensatz...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# **Den passenden Datensatz laden**\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m test_filepath \u001b[38;5;241m=\u001b[39m \u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m arff\u001b[38;5;241m.\u001b[39mloadarff(test_filepath)\n\u001b[0;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Pfad zu den gespeicherten Modellen (ggf. anpassen)\n",
    "model_dir = \"C:/Users/berkb/Desktop/Uni/Bachelor/Projekt/Scenario A1-ARFF/Scenario A1-ARFF/saved_models\"\n",
    "\n",
    "# Modelle laden\n",
    "models = {\n",
    "    \"15s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_15s.h5\")),\n",
    "    \"30s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_30s.h5\")),\n",
    "    \"60s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_60s.h5\")),\n",
    "}\n",
    "\n",
    "print(\" Alle Modelle erfolgreich geladen!\")\n",
    "\n",
    "\n",
    "for dataset_name, model in models.items():\n",
    "    print(f\"\\n Teste Modell {dataset_name} auf den passenden Datensatz...\")\n",
    "    \n",
    "    # **Den passenden Datensatz laden**\n",
    "    test_filepath = filepaths[dataset_name]\n",
    "    raw_data, meta = arff.loadarff(test_filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    X_test = df_scaled.values\n",
    "    y_test = np.array(labels)\n",
    "\n",
    "    # **Vorhersage mit dem Modell**\n",
    "    X_test_pred = model.predict(X_test, verbose=0)\n",
    "    mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)\n",
    "\n",
    "    # **Threshold aus dem Training nutzen (manuell anpassen oder berechnen)**\n",
    "    threshold = np.percentile(mse, 50)  \n",
    "\n",
    "    # **Anomalie-Erkennung**\n",
    "    y_pred = np.where(mse > threshold, 1, 0)  \n",
    "    y_true = np.where(y_test == \"VPN\", 1, 0)\n",
    "\n",
    "    # **Ergebnisse ausgeben**\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"])\n",
    "\n",
    "    print(f\"\\n Testergebnisse für {dataset_name}:\")\n",
    "    print(\"CONFUSION MATRIX:\\n\", cm)\n",
    "    print(\"CLASSIFICATION REPORT:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fa4fe-5b5a-400e-98c8-3d341d2add67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
