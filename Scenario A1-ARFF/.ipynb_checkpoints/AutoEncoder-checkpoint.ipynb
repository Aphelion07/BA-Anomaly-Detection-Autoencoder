{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c679d77f-bf43-49d9-b7ed-368d7f3eec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.10.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow Version:\", tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed4b498d-9a95-4ec9-aa92-a73002cf8b1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScenario A1-ARFF\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mTimeBasedFeatures-Dataset-15s-VPN.arff\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Daten laden\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n\u001b[0;32m     14\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass1\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _loadarff(ofile)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'"
     ]
    }
   ],
   "source": [
    "\n",
    "from scipy.io import arff\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "filepath = r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff'\n",
    "\n",
    "# Daten laden\n",
    "raw_data, meta = arff.loadarff(filepath)\n",
    "df = pd.DataFrame(raw_data)\n",
    "\n",
    "\n",
    "df['class1'] = df['class1'].apply(lambda x: x.decode('utf-8'))\n",
    "\n",
    "\n",
    "selected_features = [\n",
    "    'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "    'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "    'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "    'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "]\n",
    "df_selected = df[selected_features]\n",
    "labels = df['class1']\n",
    "\n",
    "\n",
    "df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_selected)\n",
    "\n",
    "\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Trainingsdaten Shape:\", X_train.shape)\n",
    "print(\"Testdaten Shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdce71f8-c479-481f-91f2-d17b741524f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Skalierung der Features mit StandardScaler\u001b[39;00m\n\u001b[0;32m     10\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m---> 11\u001b[0m X_train_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m)\n\u001b[0;32m     12\u001b[0m X_test_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Falls X_* noch DataFrames sind, in NumPy-Arrays umwandeln\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Skalierung der Features mit StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Falls X_* noch DataFrames sind, in NumPy-Arrays umwandeln\n",
    "if isinstance(X_train_scaled, pd.DataFrame):\n",
    "    X_train_np = X_train_scaled.values\n",
    "    X_test_np = X_test_scaled.values\n",
    "else:\n",
    "    X_train_np = X_train_scaled\n",
    "    X_test_np = X_test_scaled\n",
    "\n",
    "# Labels als NumPy-Array speichern\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Nur Non-VPN-Daten f√ºr das Training verwenden (un√ºberwachtes Lernen)\n",
    "mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "# Definition der Autoencoder-Architektur\n",
    "input_dim = X_train_norm.shape[1]\n",
    "input_layer = keras.Input(shape=(input_dim,))\n",
    "x = layers.Dense(128, activation='relu')(input_layer)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(96, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(48, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(16, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(48, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(96, activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(128, activation='relu')(x)\n",
    "output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Kompilieren des Autoencoders mit Adamax-Optimizer und Huber-Loss\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=0.0007), loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "# Callbacks f√ºr Early Stopping und Learning Rate Anpassung\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "clr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Training des Autoencoders\n",
    "history = autoencoder.fit(\n",
    "    X_train_norm, X_train_norm,\n",
    "    epochs=150,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    validation_split=0.1,\n",
    "    verbose=1,\n",
    "    callbacks=[clr, early_stopping]\n",
    ")\n",
    "\n",
    "# Vorhersage des Autoencoders auf Testdaten\n",
    "X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "\n",
    "# Berechnung des Rekonstruktionsfehlers auf Trainingsdaten\n",
    "X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "\n",
    "# Festlegen des Schwellenwerts f√ºr Anomalie-Erkennung\n",
    "threshold = np.percentile(mse_train_norm, 65)\n",
    "\n",
    "y_pred = np.where(mse > threshold, 1, 0)  # 1 = Anomalie (VPN), 0 = Normal (Non-VPN)\n",
    "y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "# Berechnung der Confusion Matrix und der Klassifikationsmetriken\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"\\nCONFUSION MATRIX:\\n\", cm)\n",
    "\n",
    "report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\",\"VPN\"])\n",
    "print(\"\\nCLASSIFICATION REPORT:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c714f15-4d62-4ec0-a2db-80b971d0117f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Starte Training f√ºr Datensatz: VPN\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.arff\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Starte Training f√ºr Datensatz: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m \u001b[43marff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadarff\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n\u001b[0;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mreplace([\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m], np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\scipy\\io\\arff\\_arffread.py:802\u001b[0m, in \u001b[0;36mloadarff\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    800\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 802\u001b[0m     ofile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    804\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _loadarff(ofile)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Scenario A1-ARFF\\\\TimeBasedFeatures-Dataset-15s-VPN.arff'"
     ]
    }
   ],
   "source": [
    "# TEST ALL MODELLS (81 VARIANTEN)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "filepaths = [\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff',\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-30s-VPN.arff',\n",
    "    r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-60s-VPN.arff'\n",
    "]\n",
    "\n",
    "bottleneck_sizes = [16, 32, 64]\n",
    "learning_rates = [0.0005, 0.0007, 0.001]\n",
    "dropout_rates = [0.1, 0.2, 0.3]\n",
    "thresholds = [50, 65, 75]\n",
    "\n",
    "best_results = {}\n",
    "\n",
    "for filepath in filepaths:\n",
    "    dataset_name = filepath.split('-')[-1].replace('.arff', '')\n",
    "    print(f\" Starte Training f√ºr Datensatz: {dataset_name}\")\n",
    "\n",
    "    raw_data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    selected_features = [\n",
    "        'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "        'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "        'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "        'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "    ]\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    X_train_np = X_train.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "\n",
    "    mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "    X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "\n",
    "    for bottleneck in bottleneck_sizes:\n",
    "        for lr in learning_rates:\n",
    "            for dropout_rate in dropout_rates:\n",
    "                # Architektur definieren\n",
    "                input_dim = X_train_norm.shape[1]\n",
    "                input_layer = keras.Input(shape=(input_dim,))\n",
    "                x = layers.Dense(128, activation='elu')(input_layer)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(96, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(48, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(bottleneck, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dense(48, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(96, activation='elu')(x)\n",
    "                x = layers.BatchNormalization()(x)\n",
    "                x = layers.Dropout(dropout_rate)(x)\n",
    "                x = layers.Dense(128, activation='elu')(x)\n",
    "                output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "                autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "                autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=lr),\n",
    "                                   loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "                autoencoder.fit(\n",
    "                    X_train_norm, X_train_norm,\n",
    "                    epochs=100,\n",
    "                    batch_size=64,\n",
    "                    shuffle=True,\n",
    "                    validation_split=0.1,\n",
    "                    verbose=0\n",
    "                )\n",
    "\n",
    "                X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "                mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "\n",
    "                X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "                mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "\n",
    "                for threshold_p in thresholds:\n",
    "                    threshold = np.percentile(mse_train_norm, threshold_p)\n",
    "                    print(f\" Testing: {dataset_name} | Bottleneck={bottleneck}, LR={lr}, Dropout={dropout_rate}, Threshold={threshold_p}%\")\n",
    "\n",
    "                    y_pred = np.where(mse > threshold, 1, 0)\n",
    "                    y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "                    cm = confusion_matrix(y_true, y_pred)\n",
    "                    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"], output_dict=True)\n",
    "                    f1_score = report[\"VPN\"][\"f1-score\"]\n",
    "\n",
    "                    if f1_score > best_f1:\n",
    "                        best_f1 = f1_score\n",
    "                        best_model = autoencoder\n",
    "                        best_params = (bottleneck, lr, dropout_rate, threshold_p)\n",
    "\n",
    "    best_results[dataset_name] = {\n",
    "        \"best_f1\": best_f1,\n",
    "        \"best_params\": best_params\n",
    "    }\n",
    "\n",
    "    print(f\"\\n Best Model for {dataset_name}:\")\n",
    "    print(f\"  - Bottleneck: {best_params[0]}\")\n",
    "    print(f\"  - Learning Rate: {best_params[1]}\")\n",
    "    print(f\"  - Dropout Rate: {best_params[2]}\")\n",
    "    print(f\"  - Threshold: {best_params[3]}%\")\n",
    "    print(f\"  - Best F1-Score: {best_f1:.4f}\")\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74221ff5-8dde-48f7-a86a-9b4a71401ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starte Training & Test f√ºr Datensatz: 15s\n",
      "Epoch 1/100\n",
      "101/101 [==============================] - 2s 10ms/step - loss: 0.1629 - val_loss: 0.0774\n",
      "Epoch 2/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0971 - val_loss: 0.0609\n",
      "Epoch 3/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0795 - val_loss: 0.0421\n",
      "Epoch 4/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0684 - val_loss: 0.0306\n",
      "Epoch 5/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0597 - val_loss: 0.0295\n",
      "Epoch 6/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0555 - val_loss: 0.0259\n",
      "Epoch 7/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0283\n",
      "Epoch 8/100\n",
      "101/101 [==============================] - 1s 7ms/step - loss: 0.0483 - val_loss: 0.0261\n",
      "Epoch 9/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0487 - val_loss: 0.0340\n",
      "Epoch 10/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0457 - val_loss: 0.0297\n",
      "Epoch 11/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0435 - val_loss: 0.0235\n",
      "Epoch 12/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0423 - val_loss: 0.0222\n",
      "Epoch 13/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0427 - val_loss: 0.0325\n",
      "Epoch 14/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0406 - val_loss: 0.0203\n",
      "Epoch 15/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0389 - val_loss: 0.0291\n",
      "Epoch 16/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0378 - val_loss: 0.0238\n",
      "Epoch 17/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0389 - val_loss: 0.0270\n",
      "Epoch 18/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0378 - val_loss: 0.0272\n",
      "Epoch 19/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0360 - val_loss: 0.0220\n",
      "Epoch 20/100\n",
      "101/101 [==============================] - 1s 7ms/step - loss: 0.0360 - val_loss: 0.0214\n",
      "Epoch 21/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0363 - val_loss: 0.0250\n",
      "Epoch 22/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0335 - val_loss: 0.0262\n",
      "Epoch 23/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0338 - val_loss: 0.0265\n",
      "Epoch 24/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0333 - val_loss: 0.0254\n",
      "Epoch 25/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0318 - val_loss: 0.0253\n",
      "Epoch 26/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0324 - val_loss: 0.0267\n",
      "Epoch 27/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0318 - val_loss: 0.0182\n",
      "Epoch 28/100\n",
      "101/101 [==============================] - 1s 9ms/step - loss: 0.0312 - val_loss: 0.0245\n",
      "Epoch 29/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0302 - val_loss: 0.0190\n",
      "Epoch 30/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0302 - val_loss: 0.0179\n",
      "Epoch 31/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0304 - val_loss: 0.0228\n",
      "Epoch 32/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0299 - val_loss: 0.0245\n",
      "Epoch 33/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0293 - val_loss: 0.0176\n",
      "Epoch 34/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0282 - val_loss: 0.0212\n",
      "Epoch 35/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0274 - val_loss: 0.0224\n",
      "Epoch 36/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0276 - val_loss: 0.0198\n",
      "Epoch 37/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0272 - val_loss: 0.0245\n",
      "Epoch 38/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0285 - val_loss: 0.0182\n",
      "Epoch 39/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0268 - val_loss: 0.0204\n",
      "Epoch 40/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0282 - val_loss: 0.0154\n",
      "Epoch 41/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0266 - val_loss: 0.0231\n",
      "Epoch 42/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0265 - val_loss: 0.0196\n",
      "Epoch 43/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0258 - val_loss: 0.0224\n",
      "Epoch 44/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0252 - val_loss: 0.0173\n",
      "Epoch 45/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0173\n",
      "Epoch 46/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0252 - val_loss: 0.0155\n",
      "Epoch 47/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0247 - val_loss: 0.0195\n",
      "Epoch 48/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0246 - val_loss: 0.0108\n",
      "Epoch 49/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0245 - val_loss: 0.0209\n",
      "Epoch 50/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0241 - val_loss: 0.0178\n",
      "Epoch 51/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0237 - val_loss: 0.0178\n",
      "Epoch 52/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0242 - val_loss: 0.0161\n",
      "Epoch 53/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0237 - val_loss: 0.0148\n",
      "Epoch 54/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0145\n",
      "Epoch 55/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0223 - val_loss: 0.0224\n",
      "Epoch 56/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0215 - val_loss: 0.0165\n",
      "Epoch 57/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0215 - val_loss: 0.0189\n",
      "Epoch 58/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0215 - val_loss: 0.0222\n",
      "Epoch 59/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0222 - val_loss: 0.0181\n",
      "Epoch 60/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0222 - val_loss: 0.0185\n",
      "Epoch 61/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0219 - val_loss: 0.0234\n",
      "Epoch 62/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0203\n",
      "Epoch 63/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0213 - val_loss: 0.0219\n",
      "Epoch 64/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0217\n",
      "Epoch 65/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0214 - val_loss: 0.0166\n",
      "Epoch 66/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0221 - val_loss: 0.0160\n",
      "Epoch 67/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0198 - val_loss: 0.0177\n",
      "Epoch 68/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0209 - val_loss: 0.0153\n",
      "Epoch 69/100\n",
      "101/101 [==============================] - 1s 9ms/step - loss: 0.0207 - val_loss: 0.0176\n",
      "Epoch 70/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0212 - val_loss: 0.0173\n",
      "Epoch 71/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0212 - val_loss: 0.0140\n",
      "Epoch 72/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0199 - val_loss: 0.0179\n",
      "Epoch 73/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0196 - val_loss: 0.0160\n",
      "Epoch 74/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0195 - val_loss: 0.0137\n",
      "Epoch 75/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0205 - val_loss: 0.0176\n",
      "Epoch 76/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0197 - val_loss: 0.0202\n",
      "Epoch 77/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0193 - val_loss: 0.0181\n",
      "Epoch 78/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0197 - val_loss: 0.0164\n",
      "Epoch 79/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0199 - val_loss: 0.0167\n",
      "Epoch 80/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0201 - val_loss: 0.0154\n",
      "Epoch 81/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0191 - val_loss: 0.0200\n",
      "Epoch 82/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0189 - val_loss: 0.0192\n",
      "Epoch 83/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0195 - val_loss: 0.0206\n",
      "Epoch 84/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0191 - val_loss: 0.0195\n",
      "Epoch 85/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0196 - val_loss: 0.0160\n",
      "Epoch 86/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0192 - val_loss: 0.0204\n",
      "Epoch 87/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0188 - val_loss: 0.0178\n",
      "Epoch 88/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0181 - val_loss: 0.0129\n",
      "Epoch 89/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0196 - val_loss: 0.0189\n",
      "Epoch 90/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0198 - val_loss: 0.0190\n",
      "Epoch 91/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0180 - val_loss: 0.0177\n",
      "Epoch 92/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0174 - val_loss: 0.0164\n",
      "Epoch 93/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0190 - val_loss: 0.0166\n",
      "Epoch 94/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0175 - val_loss: 0.0168\n",
      "Epoch 95/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0180 - val_loss: 0.0169\n",
      "Epoch 96/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0174 - val_loss: 0.0146\n",
      "Epoch 97/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0179 - val_loss: 0.0156\n",
      "Epoch 98/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0175 - val_loss: 0.0165\n",
      "Epoch 99/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0163\n",
      "Epoch 100/100\n",
      "101/101 [==============================] - 1s 8ms/step - loss: 0.0172 - val_loss: 0.0166\n",
      "\n",
      "‚úÖ Training f√ºr 15s abgeschlossen! Schwellenwert = 0.001809\n",
      "\n",
      "\n",
      "üèÜ Ergebnisse f√ºr 15s:\n",
      "CONFUSION MATRIX:\n",
      " [[ 913  880]\n",
      " [ 726 1233]]\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Non-VPN       0.56      0.51      0.53      1793\n",
      "         VPN       0.58      0.63      0.61      1959\n",
      "\n",
      "    accuracy                           0.57      3752\n",
      "   macro avg       0.57      0.57      0.57      3752\n",
      "weighted avg       0.57      0.57      0.57      3752\n",
      "\n",
      "‚úÖ Modell gespeichert: saved_models\\autoencoder_15s.h5\n",
      "\n",
      "üöÄ Starte Training & Test f√ºr Datensatz: 30s\n",
      "Epoch 1/100\n",
      "78/78 [==============================] - 2s 10ms/step - loss: 0.2957 - val_loss: 0.0876\n",
      "Epoch 2/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.1788 - val_loss: 0.0697\n",
      "Epoch 3/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.1460 - val_loss: 0.0552\n",
      "Epoch 4/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.1255 - val_loss: 0.0476\n",
      "Epoch 5/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.1143 - val_loss: 0.0413\n",
      "Epoch 6/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.1059 - val_loss: 0.0403\n",
      "Epoch 7/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0991 - val_loss: 0.0373\n",
      "Epoch 8/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0946 - val_loss: 0.0360\n",
      "Epoch 9/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0874 - val_loss: 0.0338\n",
      "Epoch 10/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0871 - val_loss: 0.0334\n",
      "Epoch 11/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0820 - val_loss: 0.0335\n",
      "Epoch 12/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0806 - val_loss: 0.0330\n",
      "Epoch 13/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0787 - val_loss: 0.0298\n",
      "Epoch 14/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0809 - val_loss: 0.0296\n",
      "Epoch 15/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0761 - val_loss: 0.0285\n",
      "Epoch 16/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0735 - val_loss: 0.0307\n",
      "Epoch 17/100\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0723 - val_loss: 0.0286\n",
      "Epoch 18/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0704 - val_loss: 0.0281\n",
      "Epoch 19/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0670 - val_loss: 0.0280\n",
      "Epoch 20/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0674 - val_loss: 0.0288\n",
      "Epoch 21/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0652 - val_loss: 0.0252\n",
      "Epoch 22/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0634 - val_loss: 0.0250\n",
      "Epoch 23/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0640 - val_loss: 0.0258\n",
      "Epoch 24/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0627 - val_loss: 0.0274\n",
      "Epoch 25/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0605 - val_loss: 0.0265\n",
      "Epoch 26/100\n",
      "78/78 [==============================] - 1s 10ms/step - loss: 0.0614 - val_loss: 0.0257\n",
      "Epoch 27/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0617 - val_loss: 0.0247\n",
      "Epoch 28/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0571 - val_loss: 0.0242\n",
      "Epoch 29/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0576 - val_loss: 0.0225\n",
      "Epoch 30/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0563 - val_loss: 0.0233\n",
      "Epoch 31/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0575 - val_loss: 0.0233\n",
      "Epoch 32/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0560 - val_loss: 0.0248\n",
      "Epoch 33/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0556 - val_loss: 0.0232\n",
      "Epoch 34/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0543 - val_loss: 0.0236\n",
      "Epoch 35/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0531 - val_loss: 0.0197\n",
      "Epoch 36/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0530 - val_loss: 0.0229\n",
      "Epoch 37/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0522 - val_loss: 0.0214\n",
      "Epoch 38/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0518 - val_loss: 0.0191\n",
      "Epoch 39/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0501 - val_loss: 0.0208\n",
      "Epoch 40/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0489 - val_loss: 0.0213\n",
      "Epoch 41/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0495 - val_loss: 0.0206\n",
      "Epoch 42/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0487 - val_loss: 0.0205\n",
      "Epoch 43/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0482 - val_loss: 0.0226\n",
      "Epoch 44/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0472 - val_loss: 0.0191\n",
      "Epoch 45/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0473 - val_loss: 0.0212\n",
      "Epoch 46/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0471 - val_loss: 0.0226\n",
      "Epoch 47/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0469 - val_loss: 0.0190\n",
      "Epoch 48/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0462 - val_loss: 0.0195\n",
      "Epoch 49/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0456 - val_loss: 0.0200\n",
      "Epoch 50/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0434 - val_loss: 0.0178\n",
      "Epoch 51/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0455 - val_loss: 0.0198\n",
      "Epoch 52/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0435 - val_loss: 0.0193\n",
      "Epoch 53/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0440 - val_loss: 0.0187\n",
      "Epoch 54/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0432 - val_loss: 0.0176\n",
      "Epoch 55/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0425 - val_loss: 0.0172\n",
      "Epoch 56/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0426 - val_loss: 0.0155\n",
      "Epoch 57/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0401 - val_loss: 0.0144\n",
      "Epoch 58/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0406 - val_loss: 0.0157\n",
      "Epoch 59/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0407 - val_loss: 0.0163\n",
      "Epoch 60/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0415 - val_loss: 0.0146\n",
      "Epoch 61/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0407 - val_loss: 0.0179\n",
      "Epoch 62/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0396 - val_loss: 0.0186\n",
      "Epoch 63/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0412 - val_loss: 0.0185\n",
      "Epoch 64/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0390 - val_loss: 0.0140\n",
      "Epoch 65/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0383 - val_loss: 0.0173\n",
      "Epoch 66/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0384 - val_loss: 0.0157\n",
      "Epoch 67/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0401 - val_loss: 0.0140\n",
      "Epoch 68/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0369 - val_loss: 0.0147\n",
      "Epoch 69/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0391 - val_loss: 0.0174\n",
      "Epoch 70/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0393 - val_loss: 0.0136\n",
      "Epoch 71/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0378 - val_loss: 0.0119\n",
      "Epoch 72/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0364 - val_loss: 0.0147\n",
      "Epoch 73/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0380 - val_loss: 0.0160\n",
      "Epoch 74/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0358 - val_loss: 0.0135\n",
      "Epoch 75/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0355 - val_loss: 0.0144\n",
      "Epoch 76/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0365 - val_loss: 0.0104\n",
      "Epoch 77/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0366 - val_loss: 0.0144\n",
      "Epoch 78/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0364 - val_loss: 0.0115\n",
      "Epoch 79/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0357 - val_loss: 0.0150\n",
      "Epoch 80/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0359 - val_loss: 0.0128\n",
      "Epoch 81/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0360 - val_loss: 0.0122\n",
      "Epoch 82/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0348 - val_loss: 0.0136\n",
      "Epoch 83/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0128\n",
      "Epoch 84/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0342 - val_loss: 0.0106\n",
      "Epoch 85/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0341 - val_loss: 0.0155\n",
      "Epoch 86/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0339 - val_loss: 0.0103\n",
      "Epoch 87/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0354 - val_loss: 0.0117\n",
      "Epoch 88/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0346 - val_loss: 0.0137\n",
      "Epoch 89/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0341 - val_loss: 0.0136\n",
      "Epoch 90/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0354 - val_loss: 0.0110\n",
      "Epoch 91/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0343 - val_loss: 0.0114\n",
      "Epoch 92/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0348 - val_loss: 0.0130\n",
      "Epoch 93/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0340 - val_loss: 0.0116\n",
      "Epoch 94/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0334 - val_loss: 0.0125\n",
      "Epoch 95/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0332 - val_loss: 0.0137\n",
      "Epoch 96/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0331 - val_loss: 0.0101\n",
      "Epoch 97/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0331 - val_loss: 0.0121\n",
      "Epoch 98/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0342 - val_loss: 0.0114\n",
      "Epoch 99/100\n",
      "78/78 [==============================] - 1s 8ms/step - loss: 0.0329 - val_loss: 0.0128\n",
      "Epoch 100/100\n",
      "78/78 [==============================] - 1s 9ms/step - loss: 0.0324 - val_loss: 0.0107\n",
      "\n",
      "‚úÖ Training f√ºr 30s abgeschlossen! Schwellenwert = 0.003425\n",
      "\n",
      "\n",
      "üèÜ Ergebnisse f√ºr 30s:\n",
      "CONFUSION MATRIX:\n",
      " [[709 675]\n",
      " [654 893]]\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Non-VPN       0.52      0.51      0.52      1384\n",
      "         VPN       0.57      0.58      0.57      1547\n",
      "\n",
      "    accuracy                           0.55      2931\n",
      "   macro avg       0.54      0.54      0.54      2931\n",
      "weighted avg       0.55      0.55      0.55      2931\n",
      "\n",
      "‚úÖ Modell gespeichert: saved_models\\autoencoder_30s.h5\n",
      "\n",
      "üöÄ Starte Training & Test f√ºr Datensatz: 60s\n",
      "Epoch 1/100\n",
      "97/97 [==============================] - 2s 9ms/step - loss: 0.2277 - val_loss: 0.1148\n",
      "Epoch 2/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.1363 - val_loss: 0.0880\n",
      "Epoch 3/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.1140 - val_loss: 0.0672\n",
      "Epoch 4/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.1024 - val_loss: 0.0598\n",
      "Epoch 5/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0950 - val_loss: 0.0545\n",
      "Epoch 6/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0867 - val_loss: 0.0495\n",
      "Epoch 7/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0830 - val_loss: 0.0520\n",
      "Epoch 8/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0745 - val_loss: 0.0373\n",
      "Epoch 9/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0735 - val_loss: 0.0334\n",
      "Epoch 10/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0688 - val_loss: 0.0261\n",
      "Epoch 11/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0660 - val_loss: 0.0296\n",
      "Epoch 12/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0598 - val_loss: 0.0249\n",
      "Epoch 13/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0576 - val_loss: 0.0315\n",
      "Epoch 14/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0549 - val_loss: 0.0170\n",
      "Epoch 15/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0529 - val_loss: 0.0180\n",
      "Epoch 16/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0542 - val_loss: 0.0210\n",
      "Epoch 17/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0521 - val_loss: 0.0198\n",
      "Epoch 18/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0486 - val_loss: 0.0161\n",
      "Epoch 19/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0484 - val_loss: 0.0187\n",
      "Epoch 20/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0469 - val_loss: 0.0167\n",
      "Epoch 21/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0472 - val_loss: 0.0186\n",
      "Epoch 22/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0451 - val_loss: 0.0157\n",
      "Epoch 23/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0435 - val_loss: 0.0241\n",
      "Epoch 24/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0435 - val_loss: 0.0237\n",
      "Epoch 25/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0421 - val_loss: 0.0235\n",
      "Epoch 26/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0423 - val_loss: 0.0198\n",
      "Epoch 27/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0407 - val_loss: 0.0189\n",
      "Epoch 28/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0412 - val_loss: 0.0127\n",
      "Epoch 29/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0403 - val_loss: 0.0267\n",
      "Epoch 30/100\n",
      "97/97 [==============================] - 1s 7ms/step - loss: 0.0405 - val_loss: 0.0220\n",
      "Epoch 31/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0406 - val_loss: 0.0376\n",
      "Epoch 32/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0379 - val_loss: 0.0293\n",
      "Epoch 33/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0371 - val_loss: 0.0238\n",
      "Epoch 34/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0391 - val_loss: 0.0175\n",
      "Epoch 35/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0370 - val_loss: 0.0148\n",
      "Epoch 36/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0349 - val_loss: 0.0210\n",
      "Epoch 37/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0350 - val_loss: 0.0170\n",
      "Epoch 38/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0349 - val_loss: 0.0131\n",
      "Epoch 39/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0347 - val_loss: 0.0287\n",
      "Epoch 40/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0340 - val_loss: 0.0192\n",
      "Epoch 41/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0327 - val_loss: 0.0290\n",
      "Epoch 42/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0337 - val_loss: 0.0180\n",
      "Epoch 43/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0318 - val_loss: 0.0272\n",
      "Epoch 44/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0321 - val_loss: 0.0127\n",
      "Epoch 45/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0338 - val_loss: 0.0113\n",
      "Epoch 46/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0317 - val_loss: 0.0189\n",
      "Epoch 47/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0311 - val_loss: 0.0133\n",
      "Epoch 48/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0314 - val_loss: 0.0179\n",
      "Epoch 49/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0331 - val_loss: 0.0292\n",
      "Epoch 50/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0299 - val_loss: 0.0201\n",
      "Epoch 51/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0311 - val_loss: 0.0197\n",
      "Epoch 52/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0299 - val_loss: 0.0217\n",
      "Epoch 53/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0297 - val_loss: 0.0229\n",
      "Epoch 54/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0291 - val_loss: 0.0115\n",
      "Epoch 55/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0301 - val_loss: 0.0229\n",
      "Epoch 56/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0298 - val_loss: 0.0262\n",
      "Epoch 57/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0299 - val_loss: 0.0268\n",
      "Epoch 58/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0299 - val_loss: 0.0268\n",
      "Epoch 59/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0302 - val_loss: 0.0236\n",
      "Epoch 60/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0286 - val_loss: 0.0144\n",
      "Epoch 61/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0276 - val_loss: 0.0212\n",
      "Epoch 62/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0284 - val_loss: 0.0231\n",
      "Epoch 63/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0277 - val_loss: 0.0164\n",
      "Epoch 64/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0284 - val_loss: 0.0328\n",
      "Epoch 65/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0276 - val_loss: 0.0249\n",
      "Epoch 66/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0271 - val_loss: 0.0176\n",
      "Epoch 67/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0266 - val_loss: 0.0180\n",
      "Epoch 68/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0275 - val_loss: 0.0105\n",
      "Epoch 69/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0273 - val_loss: 0.0133\n",
      "Epoch 70/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0246\n",
      "Epoch 71/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0257 - val_loss: 0.0212\n",
      "Epoch 72/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0254 - val_loss: 0.0212\n",
      "Epoch 73/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0265 - val_loss: 0.0221\n",
      "Epoch 74/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0265 - val_loss: 0.0177\n",
      "Epoch 75/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0262 - val_loss: 0.0212\n",
      "Epoch 76/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0255 - val_loss: 0.0226\n",
      "Epoch 77/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0253 - val_loss: 0.0228\n",
      "Epoch 78/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0253 - val_loss: 0.0200\n",
      "Epoch 79/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0250 - val_loss: 0.0145\n",
      "Epoch 80/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0253 - val_loss: 0.0223\n",
      "Epoch 81/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0239 - val_loss: 0.0198\n",
      "Epoch 82/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0256 - val_loss: 0.0183\n",
      "Epoch 83/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0244 - val_loss: 0.0241\n",
      "Epoch 84/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0243 - val_loss: 0.0172\n",
      "Epoch 85/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0235 - val_loss: 0.0152\n",
      "Epoch 86/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0232 - val_loss: 0.0200\n",
      "Epoch 87/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0234 - val_loss: 0.0140\n",
      "Epoch 88/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0245 - val_loss: 0.0127\n",
      "Epoch 89/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0232 - val_loss: 0.0290\n",
      "Epoch 90/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0247 - val_loss: 0.0179\n",
      "Epoch 91/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0228 - val_loss: 0.0153\n",
      "Epoch 92/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0226 - val_loss: 0.0192\n",
      "Epoch 93/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0174\n",
      "Epoch 94/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0252 - val_loss: 0.0210\n",
      "Epoch 95/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0225 - val_loss: 0.0189\n",
      "Epoch 96/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0225 - val_loss: 0.0110\n",
      "Epoch 97/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0236 - val_loss: 0.0205\n",
      "Epoch 98/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0229 - val_loss: 0.0200\n",
      "Epoch 99/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0221 - val_loss: 0.0177\n",
      "Epoch 100/100\n",
      "97/97 [==============================] - 1s 8ms/step - loss: 0.0221 - val_loss: 0.0214\n",
      "\n",
      "‚úÖ Training f√ºr 60s abgeschlossen! Schwellenwert = 0.002721\n",
      "\n",
      "\n",
      "üèÜ Ergebnisse f√ºr 60s:\n",
      "CONFUSION MATRIX:\n",
      " [[824 892]\n",
      " [624 763]]\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Non-VPN       0.57      0.48      0.52      1716\n",
      "         VPN       0.46      0.55      0.50      1387\n",
      "\n",
      "    accuracy                           0.51      3103\n",
      "   macro avg       0.52      0.52      0.51      3103\n",
      "weighted avg       0.52      0.51      0.51      3103\n",
      "\n",
      "‚úÖ Modell gespeichert: saved_models\\autoencoder_60s.h5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "\n",
    "best_params = {\n",
    "    \"15s\": {\"bottleneck\": 16, \"lr\": 0.001, \"dropout\": 0.1, \"threshold_p\": 50},\n",
    "    \"30s\": {\"bottleneck\": 32, \"lr\": 0.0007, \"dropout\": 0.2, \"threshold_p\": 50},\n",
    "    \"60s\": {\"bottleneck\": 16, \"lr\": 0.0005, \"dropout\": 0.1, \"threshold_p\": 50}\n",
    "}\n",
    "\n",
    "\n",
    "filepaths = {\n",
    "    \"15s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff',\n",
    "    \"30s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-30s-VPN.arff',\n",
    "    \"60s\": r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-60s-VPN.arff'\n",
    "}\n",
    "\n",
    "# Speicherort f√ºr Modelle\n",
    "model_dir = \"saved_models\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Trainiere & teste je Datensatz mit besten Hyperparametern\n",
    "for dataset_name, filepath in filepaths.items():\n",
    "    print(f\"\\n Starte Training & Test f√ºr Datensatz: {dataset_name}\")\n",
    "\n",
    "    # Beste Parameter f√ºr diesen Datensatz\n",
    "    params = best_params[dataset_name]\n",
    "    bottleneck = params[\"bottleneck\"]\n",
    "    learning_rate = params[\"lr\"]\n",
    "    dropout_rate = params[\"dropout\"]\n",
    "    threshold_p = params[\"threshold_p\"]\n",
    "\n",
    "    # Daten laden\n",
    "    raw_data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    # **Feature-Engineering**\n",
    "    selected_features = [\n",
    "        'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "        'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "        'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "        'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "    ]\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    # **Skalierung**\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    # **Train-Test-Split**\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    "    )\n",
    "\n",
    "    X_train_np = X_train.values\n",
    "    X_test_np = X_test.values\n",
    "    y_train_np = np.array(y_train)\n",
    "    y_test_np = np.array(y_test)\n",
    "\n",
    "    mask_normal_train = (y_train_np == \"Non-VPN\").astype(bool)\n",
    "    X_train_norm = X_train_np[mask_normal_train]\n",
    "\n",
    "    # **Autoencoder Architektur**\n",
    "    input_dim = X_train_norm.shape[1]\n",
    "    input_layer = keras.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(128, activation='elu')(input_layer)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(48, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(bottleneck, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dense(48, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(96, activation='elu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    x = layers.Dense(128, activation='elu')(x)\n",
    "    output_layer = layers.Dense(input_dim, activation='linear')(x)\n",
    "\n",
    "    autoencoder = keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "    autoencoder.compile(optimizer=keras.optimizers.Adamax(learning_rate=learning_rate),\n",
    "                        loss=tf.keras.losses.Huber(delta=1.0))\n",
    "\n",
    "    # Training des Autoencoders\n",
    "    autoencoder.fit(\n",
    "        X_train_norm, X_train_norm,\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        shuffle=True,\n",
    "        validation_split=0.1,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Threshold berechnen\n",
    "    X_train_norm_pred = autoencoder.predict(X_train_norm, verbose=0)\n",
    "    mse_train_norm = np.mean(np.power(X_train_norm - X_train_norm_pred, 2), axis=1)\n",
    "    threshold = np.percentile(mse_train_norm, threshold_p)\n",
    "\n",
    "    print(f\"\\n Training f√ºr {dataset_name} fertig! Schwellenwert = {threshold:.6f}\\n\")\n",
    "\n",
    "    # Modell testen\n",
    "    X_test_pred = autoencoder.predict(X_test_np, verbose=0)\n",
    "    mse = np.mean(np.power(X_test_np - X_test_pred, 2), axis=1)\n",
    "    \n",
    "    y_pred = np.where(mse > threshold, 1, 0)\n",
    "    y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"])\n",
    "\n",
    "    print(f\"\\n Ergebnisse f√ºr {dataset_name}:\")\n",
    "    print(\"CONFUSION MATRIX:\\n\", cm)\n",
    "    print(\"CLASSIFICATION REPORT:\\n\", report)\n",
    "\n",
    "    # Modell speichern\n",
    "    model_path = os.path.join(model_dir, f\"autoencoder_{dataset_name}.h5\")\n",
    "    autoencoder.save(model_path)\n",
    "    print(f\" Modell gespeichert: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3647345a-7552-4817-92fd-f2192bca543c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Alle Modelle erfolgreich geladen!\n",
      "\n",
      " Teste Modell 15s auf den passenden Datensatz...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Teste Modell \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m auf den passenden Datensatz...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# **Den passenden Datensatz laden**\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m test_filepath \u001b[38;5;241m=\u001b[39m \u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_name\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     22\u001b[0m raw_data, meta \u001b[38;5;241m=\u001b[39m arff\u001b[38;5;241m.\u001b[39mloadarff(test_filepath)\n\u001b[0;32m     23\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(raw_data)\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Pfad zu den gespeicherten Modellen (ggf. anpassen)\n",
    "model_dir = \"C:/Users/berkb/Desktop/Uni/Bachelor/Projekt/Scenario A1-ARFF/Scenario A1-ARFF/saved_models\"\n",
    "\n",
    "# Modelle laden\n",
    "models = {\n",
    "    \"15s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_15s.h5\")),\n",
    "    \"30s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_30s.h5\")),\n",
    "    \"60s\": tf.keras.models.load_model(os.path.join(model_dir, \"autoencoder_60s.h5\")),\n",
    "}\n",
    "\n",
    "print(\" Alle Modelle erfolgreich geladen!\")\n",
    "\n",
    "\n",
    "for dataset_name, model in models.items():\n",
    "    print(f\"\\n Teste Modell {dataset_name} auf den passenden Datensatz...\")\n",
    "    \n",
    "    # **Den passenden Datensatz laden**\n",
    "    test_filepath = filepaths[dataset_name]\n",
    "    raw_data, meta = arff.loadarff(test_filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "\n",
    "    df_selected = df[selected_features]\n",
    "    labels = df['class1']\n",
    "    df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df_selected)\n",
    "    df_scaled = pd.DataFrame(df_scaled, columns=selected_features)\n",
    "\n",
    "    X_test = df_scaled.values\n",
    "    y_test = np.array(labels)\n",
    "\n",
    "    # **Vorhersage mit dem Modell**\n",
    "    X_test_pred = model.predict(X_test, verbose=0)\n",
    "    mse = np.mean(np.power(X_test - X_test_pred, 2), axis=1)\n",
    "\n",
    "    # **Threshold aus dem Training nutzen (manuell anpassen oder berechnen)**\n",
    "    threshold = np.percentile(mse, 50)  \n",
    "\n",
    "    # **Anomalie-Erkennung**\n",
    "    y_pred = np.where(mse > threshold, 1, 0)  \n",
    "    y_true = np.where(y_test == \"VPN\", 1, 0)\n",
    "\n",
    "    # **Ergebnisse ausgeben**\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=[\"Non-VPN\", \"VPN\"])\n",
    "\n",
    "    print(f\"\\n Testergebnisse f√ºr {dataset_name}:\")\n",
    "    print(\"CONFUSION MATRIX:\\n\", cm)\n",
    "    print(\"CLASSIFICATION REPORT:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8fa4fe-5b5a-400e-98c8-3d341d2add67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
