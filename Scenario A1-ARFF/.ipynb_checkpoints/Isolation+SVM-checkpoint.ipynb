{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9296b3f-25b1-4e0d-adcf-cbe7d8557a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Training Isolation Forest...\n",
      "\n",
      "üèÜ Ergebnisse f√ºr Isolation Forest:\n",
      "CONFUSION MATRIX:\n",
      " [[1024  769]\n",
      " [ 873 1086]]\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Non-VPN       0.54      0.57      0.56      1793\n",
      "         VPN       0.59      0.55      0.57      1959\n",
      "\n",
      "    accuracy                           0.56      3752\n",
      "   macro avg       0.56      0.56      0.56      3752\n",
      "weighted avg       0.56      0.56      0.56      3752\n",
      "\n",
      "\n",
      "üîπ Training One-Class SVM...\n",
      "\n",
      "üèÜ Ergebnisse f√ºr One-Class SVM:\n",
      "CONFUSION MATRIX:\n",
      " [[1619  174]\n",
      " [1674  285]]\n",
      "CLASSIFICATION REPORT:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Non-VPN       0.49      0.90      0.64      1793\n",
      "         VPN       0.62      0.15      0.24      1959\n",
      "\n",
      "    accuracy                           0.51      3752\n",
      "   macro avg       0.56      0.52      0.44      3752\n",
      "weighted avg       0.56      0.51      0.43      3752\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from scipy.io import arff\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.pipeline import Pipeline\n",
    "import os\n",
    "\n",
    "def load_data(filepath):\n",
    "    if not os.path.exists(filepath):\n",
    "        raise FileNotFoundError(f\"Die Datei {filepath} wurde nicht gefunden.\")\n",
    "    \n",
    "    raw_data, meta = arff.loadarff(filepath)\n",
    "    df = pd.DataFrame(raw_data)\n",
    "    df.replace([b'', ''], np.nan, inplace=True)\n",
    "    for col in df.select_dtypes([object]):\n",
    "        df[col] = df[col].apply(lambda x: x.decode('utf-8') if isinstance(x, bytes) else x)\n",
    "    df['class1'] = df['class1'].astype(str)\n",
    "    return df\n",
    "\n",
    "# Datensatzpfad\n",
    "filepath = r'Scenario A1-ARFF\\TimeBasedFeatures-Dataset-15s-VPN.arff'\n",
    "\n",
    "try:\n",
    "    df = load_data(filepath)\n",
    "except Exception as e:\n",
    "    print(f\"Fehler beim Laden der Datei: {e}\")\n",
    "    raise\n",
    "\n",
    "# Feature-Auswahl\n",
    "selected_features = [\n",
    "    'duration', 'total_fiat', 'total_biat', 'min_fiat', 'max_fiat',\n",
    "    'min_flowiat', 'max_flowiat', 'mean_flowiat', 'std_flowiat',\n",
    "    'mean_active', 'mean_idle', 'std_active', 'std_idle',\n",
    "    'flowBytesPerSecond', 'flowPktsPerSecond'\n",
    "]\n",
    "df_selected = df[selected_features]\n",
    "labels = df['class1']\n",
    "\n",
    "# Fehlende Werte auff√ºllen\n",
    "df_selected = df_selected.fillna(df_selected.median())\n",
    "\n",
    "# Daten skalieren und Pipeline erstellen\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "df_scaled = pd.DataFrame(pipeline.fit_transform(df_selected), columns=selected_features)\n",
    "\n",
    "# Train-Test-Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df_scaled, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "y_train_np = np.array(y_train)\n",
    "y_test_np = np.array(y_test)\n",
    "\n",
    "# Isolation Forest\n",
    "print(\"\\n Training Isolation Forest...\")\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.5, random_state=42)\n",
    "iso_forest.fit(X_train)\n",
    "y_pred_iso = iso_forest.predict(X_test)\n",
    "y_pred_iso = np.where(y_pred_iso == 1, 0, 1)  # 1 = Anomalie (VPN), 0 = Normal (Non-VPN)\n",
    "y_true = np.where(y_test_np == \"VPN\", 1, 0)\n",
    "\n",
    "print(\"\\n Ergebnisse f√ºr Isolation Forest:\")\n",
    "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(y_true, y_pred_iso))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_true, y_pred_iso, target_names=[\"Non-VPN\", \"VPN\"]))\n",
    "\n",
    "# One-Class SVM\n",
    "print(\"\\n Training One-Class SVM...\")\n",
    "one_class_svm = OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=\"scale\")\n",
    "one_class_svm.fit(X_train)\n",
    "y_pred_svm = one_class_svm.predict(X_test)\n",
    "y_pred_svm = np.where(y_pred_svm == 1, 0, 1) \n",
    "\n",
    "print(\"\\n Ergebnisse f√ºr One-Class SVM:\")\n",
    "print(\"CONFUSION MATRIX:\\n\", confusion_matrix(y_true, y_pred_svm))\n",
    "print(\"CLASSIFICATION REPORT:\\n\", classification_report(y_true, y_pred_svm, target_names=[\"Non-VPN\", \"VPN\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce76409-48b6-489c-9690-b6a9e490a10c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_gpu)",
   "language": "python",
   "name": "tf_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
